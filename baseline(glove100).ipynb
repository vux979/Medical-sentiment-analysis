{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本情感分类：使用卷积神经网络（textCNN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:25.733783Z",
     "start_time": "2022-03-14T14:37:25.029615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0 cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import  torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_ROOT = \"../../data\"\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:27.556020Z",
     "start_time": "2022-03-14T14:37:27.270465Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:28.304544Z",
     "start_time": "2022-03-14T14:37:28.286590Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadfile():\n",
    "    neg=pd.read_excel('D:/学习/研一下/大数据概论/neg_30000.xls',header=None)\n",
    "    pos=pd.read_excel('D:/学习/研一下/大数据概论/pos_30000.xls',header=None)\n",
    "\n",
    "    combined=np.concatenate((pos[0], neg[0]))\n",
    "    # print(type(pos[0][0])) <str>\n",
    "    y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))\n",
    "    # pos 1, neg 0\n",
    "\n",
    "    return combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:29.312406Z",
     "start_time": "2022-03-14T14:37:29.294468Z"
    }
   },
   "outputs": [],
   "source": [
    "#构造数据\n",
    "def data_classfier():\n",
    "    combined,y = loadfile()\n",
    "    data = combined\n",
    "    labels = y\n",
    "    print('Shape of data tensor:', len(data))\n",
    "    print('Shape of label tensor:', len(labels))\n",
    "    \n",
    "\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)  #打乱\n",
    "    data = data[indices]\n",
    "\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:30.602707Z",
     "start_time": "2022-03-14T14:37:29.830335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: 60000\n",
      "Shape of label tensor: 60000\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val = data_classfier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:30.618697Z",
     "start_time": "2022-03-14T14:37:30.605703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "48000\n",
      "48000\n",
      "12000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_val))\n",
    "print(type(y_val))\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:30.728126Z",
     "start_time": "2022-03-14T14:37:30.620677Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in  range(len(x_train)):\n",
    "    train_data.append([])\n",
    "    train_data[i].append(x_train[i])\n",
    "    train_data[i].append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:31.053178Z",
     "start_time": "2022-03-14T14:37:31.039215Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in  range(len(x_val)):\n",
    "    test_data.append([])\n",
    "    test_data[i].append(x_val[i])\n",
    "    test_data[i].append(y_val[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序最大池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:31.802902Z",
     "start_time": "2022-03-14T14:37:31.790477Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "         # x shape: (batch_size, channel, seq_len)\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:44.273046Z",
     "start_time": "2022-03-14T14:37:36.151098Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab = d2l.get_vocab_imdb(train_data)\n",
    "train_set = Data.TensorDataset(*d2l.preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*d2l.preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textCNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:44.287770Z",
     "start_time": "2022-03-14T14:37:44.275802Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        '''\n",
    "        torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0,\n",
    "        scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
    "        num_embeddings (int) – size of the dictionary of embeddings\n",
    "        embedding_dim (int) – the size of each embedding vector\n",
    "        '''\n",
    "        # 不参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        '''\n",
    "        ModuleList Holds submodules in a list.\n",
    "        '''\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 将两个形状是(批量大小, 词数, 词向量维度)的嵌入层的输出按词向量连结\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch, seq_len, 2*embed_size)\n",
    "        # 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维(即词向量那一维)，变换到前一维\n",
    "        embeddings = embeddings.permute(0, 2, 1) # 交换维度的函数\n",
    "        # 对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的\n",
    "        # Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
    "        encoding = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:44.350696Z",
     "start_time": "2022-03-14T14:37:44.290771Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载预训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:44.984156Z",
     "start_time": "2022-03-14T14:37:44.352623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15341 oov words.\n",
      "There are 15341 oov words.\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=\"/Users/tangshusen/Datasets/glove\")\n",
    "net.embedding.weight.data.copy_(\n",
    "    d2l.load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "# itos（） Returns：List mapping indices to tokens.\n",
    "\"\"\"load_pretrained_embedding从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "net.constant_embedding.weight.data.copy_(\n",
    "    d2l.load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练并评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:44.999845Z",
     "start_time": "2022-03-14T14:37:44.987149Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(data_iter, desc ='evaluate'):\n",
    "            y = torch.tensor(y, dtype=torch.long)# 更改label类型\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:37:45.031489Z",
     "start_time": "2022-03-14T14:37:45.001840Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        print(time.strftime(\"%Y-%m-%d \\t %H:%M:%S\", time.localtime()), end='\\t')\n",
    "        for X, y in tqdm(train_iter, desc ='train'):\n",
    "            X = X.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long)# 更改label类型\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "#             print(\"saving\", end='\\t')\n",
    "#             PATH = \"./SaveModel/Glove_yelp_\" + str(best_acc) + \".pth\"\n",
    "#             torch.save(net, PATH) \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:42:33.127289Z",
     "start_time": "2022-03-14T14:37:45.033484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "2022-03-14 \t 22:37:46\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|                                                                                | 0/750 [00:00<?, ?it/s]d:\\anaconda3\\envs\\torchtext0.4gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "train: 100%|██████████████████████████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.26it/s]\n",
      "evaluate:   0%|                                                                             | 0/188 [00:00<?, ?it/s]d:\\anaconda3\\envs\\torchtext0.4gpu\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "evaluate: 100%|███████████████████████████████████████████████████████████████████| 188/188 [00:04<00:00, 37.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.2338, train acc 0.901, test acc 0.951, time 57.6 sec\n",
      "2022-03-14 \t 22:38:44\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.37it/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████████████████████| 188/188 [00:04<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 0.0615, train acc 0.954, test acc 0.962, time 57.2 sec\n",
      "2022-03-14 \t 22:39:41\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.38it/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████████████████████| 188/188 [00:04<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.0270, train acc 0.971, test acc 0.952, time 57.1 sec\n",
      "2022-03-14 \t 22:40:38\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.34it/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████████████████████| 188/188 [00:04<00:00, 37.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.0136, train acc 0.980, test acc 0.960, time 57.3 sec\n",
      "2022-03-14 \t 22:41:35\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.36it/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████████████████████| 188/188 [00:05<00:00, 37.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.0079, train acc 0.986, test acc 0.962, time 57.2 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:42:33.142246Z",
     "start_time": "2022-03-14T14:42:33.130279Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.predict_sentiment(net, vocab, ['this', 'hospital', 'is', 'so', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:42:33.157207Z",
     "start_time": "2022-03-14T14:42:33.144242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.predict_sentiment(net, vocab, ['this', 'hospital', 'is', 'so', 'bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T14:47:24.520480Z",
     "start_time": "2022-03-14T14:42:33.162194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入一句医疗评价的英文：\n",
      "I feel that this hospital is very general and the charge is very expensive. I won't come next time.\n",
      "['I', 'feel', 'that', 'this', 'hospital', 'is', 'very', 'general', 'and', 'the', 'charge', 'is', 'very', 'expensive.', 'I', \"won't\", 'come', 'next', 'time.']\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "print(\"请输入一句医疗评价的英文：\")\n",
    "s = input() \n",
    "s = [i for i in s.split()]\n",
    "print(s)\n",
    "print(d2l.predict_sentiment(net, vocab, s))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
