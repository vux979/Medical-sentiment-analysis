{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220dca4d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import库\" data-toc-modified-id=\"import库-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import库</a></span></li><li><span><a href=\"#导入数据\" data-toc-modified-id=\"导入数据-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>导入数据</a></span></li><li><span><a href=\"#导入bert\" data-toc-modified-id=\"导入bert-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>导入bert</a></span></li><li><span><a href=\"#数据处理\" data-toc-modified-id=\"数据处理-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>数据处理</a></span></li><li><span><a href=\"#模型定义\" data-toc-modified-id=\"模型定义-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>模型定义</a></span></li><li><span><a href=\"#测试模型\" data-toc-modified-id=\"测试模型-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>测试模型</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6455c83",
   "metadata": {},
   "source": [
    "# import库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46592fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:42.739994Z",
     "start_time": "2022-03-11T12:04:41.470925Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu111 cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import  torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2441272",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5345b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:42.755602Z",
     "start_time": "2022-03-11T12:04:42.741989Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def loadfile():\n",
    "    neg=pd.read_excel('D:/学习/研一下/大数据概论/neg_30000.xls',header=None)\n",
    "    pos=pd.read_excel('D:/学习/研一下/大数据概论/pos_30000.xls',header=None)\n",
    "\n",
    "    combined=np.concatenate((pos[0], neg[0]))\n",
    "    # print(type(pos[0][0])) <str>\n",
    "    y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))\n",
    "    # pos 1, neg 0\n",
    "\n",
    "    return combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c52d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:42.785909Z",
     "start_time": "2022-03-11T12:04:42.771809Z"
    }
   },
   "outputs": [],
   "source": [
    "#构造数据\n",
    "def data_classfier():\n",
    "    combined,y = loadfile()\n",
    "    data = combined\n",
    "    labels = y\n",
    "    print('Shape of data tensor:', len(data))\n",
    "    print('Shape of label tensor:', len(labels))\n",
    "    \n",
    "\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)  #打乱\n",
    "    data = data[indices]\n",
    "\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1031811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:43.765884Z",
     "start_time": "2022-03-11T12:04:42.787633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: 60000\n",
      "Shape of label tensor: 60000\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val = data_classfier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdad0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:43.780809Z",
     "start_time": "2022-03-11T12:04:43.767844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "48000\n",
      "48000\n",
      "12000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_val))\n",
    "print(type(y_val))\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874a6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:43.795804Z",
     "start_time": "2022-03-11T12:04:43.782812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I originally went to Dr Shivananjappa's office after reading all of the reviews from acne patients as I have been struggling with adult acne.  The first time I called the office to make an appt. she actually got on the phone with me as I explained my issues and asked many questions about previous remedies and procedures I had tried over the years.  She spent almost an hour on the phone with me and was so personable yet very professional sharing with me what was possible and what may not be, etc.  I was very impressed with her after meeting for my first appointment and ended up staying after it chatting with her for over 45 mins about other things (like pets!).  She is a very warm yet professional person that made me feel at ease.  I actually ended up going back for a coolsculpt procedure and I simply cannot say enough about this wonderful person as well as how pleased I am with my results.  I'm lucky I live very close to her office but I would encourage anyone considering a procedure to go with Dr Shivananjappa even if you have to drive far!  I have never had such a personable experience with ANY doctor I have ever visited for anything.  She is truly a remarkable person and professional that truly cares about her patients.  Two thumbs up, 5 stars and my gratitude for the treatments she performed that have had such a positive effect on me.\"\n",
      " 'The rating I would give this place would be an infinite negative. The amount of disdain I have for one hospital is unbearable. Nobody should bring anyone they love to this place. EVER! While I think people hold the Brigham on a pedestal because of its name, this hospital is NOTHING like it used to be. The theory of \"too many hands in the pot\" holds too much truth here. My grandmother went in with an ischemic stroke, (was transferred there because her cardiologist was part of the Brigham and she had a cardiac history) was doing phenomenal and standing and moving on day 3. They were supposed to urgently replace a NG tube for a g tube since she couldn\\'t swallow and waited 6 days because \"it\\'s a holiday weekend and we are understaffed\" and the staff they had (specifically floor 10 and very specifically nurse Nicole) were HORRENDOUS. Didn\\'t move her, sit her up, wash the markings from her head after brain scan, left her to accumulate fluid causing her first round of pneumonia. Ended up in ICU who nursed her back to health, another weekend came and all the while moved her again down to nurse Nicole whom caused the same thing again. (Note: I had complained and called patient relations and they couldn\\'t even tell me who her doctor or point person was, they said \"admittedly there too many involved\" yet, never once contacted her cardiologist which is why she was there despite our many questions on that and then after she passed sent a BS letter with false information to try and cover up this event).Two weeks into the nightmare and never any scans showing blood in her brain, she showed trace blood and the same day had a cardiac event. They then treated her with heparin drip. A HEPARIN drip. Now, I\\'m not a doctor but even I know you don\\'t give someone heparin with any bleed never mind a brain bleed. There are many options. Brigham and women\\'s single handedly caused the internal downfall of my dear grandmothers organs resulting in her life. These people treat our loved ones as experiments and testing tools for students. Now, I know some of the doctors and some nurses are great. But from what I\\'ve seen on this side of things is absolutely disgusting. I work in the medical field, don\\'t trust anyone, be your own advocate, stand up for the ones you love and please for the love of all things, do NOT go to this hospital.'\n",
      " \"Staff is friendly and all, but when I have to wait the same amount of time as they tell walk ins when I have an appointment, that's annoying. Came in for a simple blood draw after a poke at work, took me back 20 min late, waited an additional 30 min in the room for the doc to review what we were doing which took 2 minutes, then 30+ min for someone to come back in for the blood draw! It's ridiculous!! I had a 9 am appointment and didn't leave until 10:20! For a blood draw!On a positive note, the phlebotomist was awesome!\"\n",
      " \"The optometrist I saw twice here is stiff, superior, condescending. I've had glasses since I was 10 years old and she wins the prize for worst bedside manner. I have great insurance, am dropping an extra $90 to get a contact lens fitting, then purchasing $800 worth of glasses. Why the 'tude? I'm being nice. You too can be nice. Don't sneer at my questions. If you hate your job do something else. Jesus. 2 points instead of 1 for the front desk staff, who were polite, although when I called ahead to see if they were running on time, did not tell me I'd be waiting for 30 min.**Edited to a 1 star as optometrist refuses to approve my 1-800-CONTACTS prescription unless I make a new appointment with her. Even though the prescription is still good for a few more weeks. Just so I can pay a visit fee to argue her into approving my contacts order, which is the brand I want, instead of the one she wants me to have. Which is uncomfortable and dries my eyes out. Walking over a bed of nails and Legos barefoot would be less painful than going to this store again.\"\n",
      " \"I can definitely recommend this medical home: they take good care of you and provide a wellness exam yearly. There are many doctors here, and from my experience they have all been knowledgeable, caring and kind; but, of course, mine is the best! ;0)I have been coming here for about three years now, almost as long as I have lived in Portland. Providence was my first (and so far, only) medical group. I have gone from the St. Vincent medical home to this North Providence medical home, and I really prefer the continuity I get from the doctor I have now. At St. Vincent's I received a new intern every year. My doctor now checks in with me on all my previous health issues, facilitated by computer programming -- which I personally like, this future of electronic access to medical history and the ease of operation with various apps in your life. Otherwise I wouldn't be a yelp elite!For urgent visits I have been here as well, with one of my children. He was treated fast and professionally, and has healed well. My doctor always seems to be up on the latest developments in medicine, and I trust in this medical home.I have been fortunately blessed with my health issues, so I have nothing to complain of. Let's hope that when things become more difficult, I will be treated equally well.\"]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce38f8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:43.810439Z",
     "start_time": "2022-03-11T12:04:43.799469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76a904",
   "metadata": {},
   "source": [
    "# 导入bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd3d62a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:04:58.381939Z",
     "start_time": "2022-03-11T12:04:43.814434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "# 这里我们调用bert-base模型，同时模型的词典经过小写处理\n",
    "model_name = 'bert-base-uncased'\n",
    "# 读取模型对应的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir='./transformers/bert-base-uncased/')\n",
    "# 载入模型\n",
    "model = BertModel.from_pretrained(model_name, cache_dir='./transformers/bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61377bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.496792Z",
     "start_time": "2022-03-11T12:04:58.385928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5b62d",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3361ab48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.667537Z",
     "start_time": "2022-03-11T12:05:14.501780Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in  range(len(x_train)):\n",
    "    train_data.append([])\n",
    "    train_data[i].append(x_train[i])\n",
    "    train_data[i].append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a67dc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.683526Z",
     "start_time": "2022-03-11T12:05:14.670530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"I originally went to Dr Shivananjappa's office after reading all of the reviews from acne patients as I have been struggling with adult acne.  The first time I called the office to make an appt. she actually got on the phone with me as I explained my issues and asked many questions about previous remedies and procedures I had tried over the years.  She spent almost an hour on the phone with me and was so personable yet very professional sharing with me what was possible and what may not be, etc.  I was very impressed with her after meeting for my first appointment and ended up staying after it chatting with her for over 45 mins about other things (like pets!).  She is a very warm yet professional person that made me feel at ease.  I actually ended up going back for a coolsculpt procedure and I simply cannot say enough about this wonderful person as well as how pleased I am with my results.  I'm lucky I live very close to her office but I would encourage anyone considering a procedure to go with Dr Shivananjappa even if you have to drive far!  I have never had such a personable experience with ANY doctor I have ever visited for anything.  She is truly a remarkable person and professional that truly cares about her patients.  Two thumbs up, 5 stars and my gratitude for the treatments she performed that have had such a positive effect on me.\", 1], ['The rating I would give this place would be an infinite negative. The amount of disdain I have for one hospital is unbearable. Nobody should bring anyone they love to this place. EVER! While I think people hold the Brigham on a pedestal because of its name, this hospital is NOTHING like it used to be. The theory of \"too many hands in the pot\" holds too much truth here. My grandmother went in with an ischemic stroke, (was transferred there because her cardiologist was part of the Brigham and she had a cardiac history) was doing phenomenal and standing and moving on day 3. They were supposed to urgently replace a NG tube for a g tube since she couldn\\'t swallow and waited 6 days because \"it\\'s a holiday weekend and we are understaffed\" and the staff they had (specifically floor 10 and very specifically nurse Nicole) were HORRENDOUS. Didn\\'t move her, sit her up, wash the markings from her head after brain scan, left her to accumulate fluid causing her first round of pneumonia. Ended up in ICU who nursed her back to health, another weekend came and all the while moved her again down to nurse Nicole whom caused the same thing again. (Note: I had complained and called patient relations and they couldn\\'t even tell me who her doctor or point person was, they said \"admittedly there too many involved\" yet, never once contacted her cardiologist which is why she was there despite our many questions on that and then after she passed sent a BS letter with false information to try and cover up this event).Two weeks into the nightmare and never any scans showing blood in her brain, she showed trace blood and the same day had a cardiac event. They then treated her with heparin drip. A HEPARIN drip. Now, I\\'m not a doctor but even I know you don\\'t give someone heparin with any bleed never mind a brain bleed. There are many options. Brigham and women\\'s single handedly caused the internal downfall of my dear grandmothers organs resulting in her life. These people treat our loved ones as experiments and testing tools for students. Now, I know some of the doctors and some nurses are great. But from what I\\'ve seen on this side of things is absolutely disgusting. I work in the medical field, don\\'t trust anyone, be your own advocate, stand up for the ones you love and please for the love of all things, do NOT go to this hospital.', 0], [\"Staff is friendly and all, but when I have to wait the same amount of time as they tell walk ins when I have an appointment, that's annoying. Came in for a simple blood draw after a poke at work, took me back 20 min late, waited an additional 30 min in the room for the doc to review what we were doing which took 2 minutes, then 30+ min for someone to come back in for the blood draw! It's ridiculous!! I had a 9 am appointment and didn't leave until 10:20! For a blood draw!On a positive note, the phlebotomist was awesome!\", 0], [\"The optometrist I saw twice here is stiff, superior, condescending. I've had glasses since I was 10 years old and she wins the prize for worst bedside manner. I have great insurance, am dropping an extra $90 to get a contact lens fitting, then purchasing $800 worth of glasses. Why the 'tude? I'm being nice. You too can be nice. Don't sneer at my questions. If you hate your job do something else. Jesus. 2 points instead of 1 for the front desk staff, who were polite, although when I called ahead to see if they were running on time, did not tell me I'd be waiting for 30 min.**Edited to a 1 star as optometrist refuses to approve my 1-800-CONTACTS prescription unless I make a new appointment with her. Even though the prescription is still good for a few more weeks. Just so I can pay a visit fee to argue her into approving my contacts order, which is the brand I want, instead of the one she wants me to have. Which is uncomfortable and dries my eyes out. Walking over a bed of nails and Legos barefoot would be less painful than going to this store again.\", 0], [\"I can definitely recommend this medical home: they take good care of you and provide a wellness exam yearly. There are many doctors here, and from my experience they have all been knowledgeable, caring and kind; but, of course, mine is the best! ;0)I have been coming here for about three years now, almost as long as I have lived in Portland. Providence was my first (and so far, only) medical group. I have gone from the St. Vincent medical home to this North Providence medical home, and I really prefer the continuity I get from the doctor I have now. At St. Vincent's I received a new intern every year. My doctor now checks in with me on all my previous health issues, facilitated by computer programming -- which I personally like, this future of electronic access to medical history and the ease of operation with various apps in your life. Otherwise I wouldn't be a yelp elite!For urgent visits I have been here as well, with one of my children. He was treated fast and professionally, and has healed well. My doctor always seems to be up on the latest developments in medicine, and I trust in this medical home.I have been fortunately blessed with my health issues, so I have nothing to complain of. Let's hope that when things become more difficult, I will be treated equally well.\", 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b861ca0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.714877Z",
     "start_time": "2022-03-11T12:05:14.685489Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in  range(len(x_val)):\n",
    "    test_data.append([])\n",
    "    test_data[i].append(x_val[i])\n",
    "    test_data[i].append(y_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b907584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.730975Z",
     "start_time": "2022-03-11T12:05:14.716881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I have been visiting Aesthetica Med Spa for all  my skin needs for years- with wonderful results! Kim is fabulous with Botox injections, a true artist, and Christina has made my face youthful and glowing again. I highly recommend Aesthetica for their team of skilled professionals and reasonable prices- get on their mailing list and you will receive information of monthly specials.', 1], [\"Absolutely Amazing! I don't have words for how professional and intelligent and talented and kind the staff and Dr. Loftus was. My dad went in for surgery this morning the doctor was finished in an hour and he was more that accommodating to my family and myself. The Nurse Practitioner Michelle Gregory went out of her way to personally write and sign doctors notes for myself and my siblings for our jobs, with no questions asked. I would highly highly recommend this practice!\", 1], ['Avoid: doctor cares more for his financial well being than your well being I never thought I\\'d write a negative review for a doctor.  However my wife and I have had a terrible experience with the doctor and their billing practice.  The short of it is, the doctor moved locations and because the new location was much less convenient, we canceled our appointment.  We spoke to them twice before the date of the appointment, and both times requested to cancel the appointment.  Doctor\\'s practice did not cancel the appointment and then charged us a no-show fee.We called to ask them to remove this bill as it was inaccurate.  We called multiple times and left messages, but no one returned a call.  Finally after our frustration was mounting, the doctor himself called.Instead of listening to us, it was clear from the beginning that the doctor had it in mind to \"tell us off.\"  The call was a formality merely.  He asked if the call was being recorded, and when I told him no - he proceeded to tell us how his staff is never wrong and that it\\'s our problem.  He was curt and condescending in his tone - and he clearly did not really care.  When I mentioned that we canceled because their practice had moved locations, he did not seem to care.  As long as he got his money, even for billing us wrongly, it did not matter to him.  He proceeded to hang up on me as well.  He said his piece, he told me off, and he had no further need of wasting his precious billable time on me anymore.I would like to call out the front desk staff is very kind and patient. Excellent bedside manners, so to speak.  However the doctor does not embody these traits as well as his staff.  He could learn from his staff.It\\'s unfortunate that our medical system places the doctor\\'s financial growth at the opposite end of patient well being.  If you want a doctor that cares for you and bills you for services actually rendered, this is not your doctor.  There are many more in and around Needham and Wellesley that I can recommend.', 0], ['Love Dr. Feil! The office is always on time and friendly.  Very comfortable environment.', 1], [\"Dr. Gorin is fabulous! I came here after my aunt's breast augmentation for a consultation. He answered all of my questions honestly. Additionally, I got my Botox done here before I moved away and it always looked great. Never TOO much - just perfect.\", 1]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba704a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:05:14.747002Z",
     "start_time": "2022-03-11T12:05:14.732931Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretreatment(original_data):\n",
    "    i = 0\n",
    "    for element in tqdm(original_data):\n",
    "        temporary = []\n",
    "        original_data[i][0] = torch.tensor(tokenizer.encode(element[0], add_special_tokens=True, max_length = 512, padding='max_length', truncation=True))\n",
    "        temporary.append(element[1])\n",
    "        original_data[i][1] = torch.tensor(temporary)\n",
    "        i = i+1\n",
    "    features =  torch.cat([original_data[i][0].unsqueeze(0).long() for i in range(len(original_data))])\n",
    "    labels =  torch.cat( [original_data[i][1].long() for i in range(len(original_data))], 0)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d39327a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.385670Z",
     "start_time": "2022-03-11T12:05:14.751985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 48000/48000 [03:29<00:00, 229.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 12000/12000 [00:49<00:00, 243.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(*(pretreatment(train_data)))\n",
    "test_set = Data.TensorDataset(*(pretreatment(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee75439f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.400634Z",
     "start_time": "2022-03-11T12:09:34.387715Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_iter = Data.DataLoader(train_set, batch_size)# , shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)# , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfc661",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad6b9758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.415594Z",
     "start_time": "2022-03-11T12:09:34.403625Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "         # x shape: (batch_size, channel, seq_len)\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e1925d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.430691Z",
     "start_time": "2022-03-11T12:09:34.418585Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # 不参与训练的嵌入层\n",
    "        # self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder_1 = nn.Linear(sum(num_channels), 256)\n",
    "        self.decoder_2 = nn.Linear(256, 2)\n",
    "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        \n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = model(inputs)[0] #shape（512， 768）\n",
    "        embeddings = outputs # (batch, seq_len, embed_size)\n",
    "        # 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维(即词向量那一维)，变换到前一维\n",
    "        embeddings = embeddings.permute(0, 2, 1) # 交换维度的函数\n",
    "        # 对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的\n",
    "        # Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
    "        encoding = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        middle = self.decoder_1(self.dropout(encoding))\n",
    "        \n",
    "        outputs = self.decoder_2(self.dropout(F.relu(middle)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77bcb15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.509189Z",
     "start_time": "2022-03-11T12:09:34.432548Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 768, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79af6052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.538844Z",
     "start_time": "2022-03-11T12:09:34.511706Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(data_iter, desc ='evaluate'):\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34d68286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T12:09:34.554711Z",
     "start_time": "2022-03-11T12:09:34.541757Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        print(time.strftime(\"%Y-%m-%d \\t %H:%M:%S\", time.localtime()), end='\\t')\n",
    "        for X, y in tqdm(train_iter, desc ='train'):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            print(\"saving\", end='\\t')\n",
    "            PATH = \"./SaveModel/BertBaseEpoch10_yelp_\" + str(best_acc) + \".pth\"\n",
    "            torch.save(net, PATH) \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f60cf6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T07:56:20.860845Z",
     "start_time": "2022-03-11T12:09:34.558700Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "2022-03-11 \t 20:09:34\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████████████████████| 24000/24000 [3:35:00<00:00,  1.86it/s]\n",
      "evaluate: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [20:24<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\tepoch 1, loss 0.2184, train acc 0.929, test acc 0.962, time 14125.2 sec\n",
      "2022-03-12 \t 00:04:59\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████████████████████| 24000/24000 [3:34:41<00:00,  1.86it/s]\n",
      "evaluate: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [20:23<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\tepoch 2, loss 0.0888, train acc 0.945, test acc 0.964, time 14104.9 sec\n",
      "2022-03-12 \t 04:00:04\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████████████████████| 24000/24000 [3:34:09<00:00,  1.87it/s]\n",
      "evaluate: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [20:21<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\tepoch 3, loss 0.0580, train acc 0.949, test acc 0.969, time 14071.2 sec\n",
      "2022-03-12 \t 07:54:35\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████████████████████| 24000/24000 [3:37:04<00:00,  1.84it/s]\n",
      "evaluate: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [20:23<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.0420, train acc 0.951, test acc 0.965, time 14248.5 sec\n",
      "2022-03-12 \t 11:52:04\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████████████████████| 24000/24000 [3:37:03<00:00,  1.84it/s]\n",
      "evaluate: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [21:27<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.0325, train acc 0.952, test acc 0.963, time 14311.1 sec\n",
      "2022-03-12 \t 15:50:35\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   3%|█▋                                                                | 618/24000 [05:44<3:37:20,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1.DES\\AppData\\Local\\Temp/ipykernel_3520/2029587743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ADMINI~1.DES\\AppData\\Local\\Temp/ipykernel_3520/3719975984.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, test_iter, net, loss, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1.DES\\AppData\\Local\\Temp/ipykernel_3520/4059003112.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#shape（500， 768）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;31m# (batch, seq_len, embed_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维(即词向量那一维)，变换到前一维\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m         )\n\u001b[1;32m--> 971\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    566\u001b[0m                 )\n\u001b[0;32m    567\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     ):\n\u001b[1;32m--> 387\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\pytorch191\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key_query\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 10\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f346db",
   "metadata": {},
   "source": [
    "# 测试模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167698c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T15:13:24.970872Z",
     "start_time": "2022-01-19T15:13:24.904764Z"
    }
   },
   "source": [
    "这个使用的权重不是保存的最好权重而是最后训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d290081e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T07:57:16.463007Z",
     "start_time": "2022-03-12T07:57:16.446066Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor(tokenizer.encode(s, add_special_tokens=True, max_length = 512, padding='max_length', truncation=True), device=device)\n",
    "    result = net(sentence.view((1, -1)))\n",
    "    m = nn.Softmax(dim=1)\n",
    "    print(m(result))\n",
    "    label = torch.argmax(result, dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e157a3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T07:58:20.408819Z",
     "start_time": "2022-03-12T07:58:17.994556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入一句与医疗相关的英文：\n",
      "I feel that this hospital is average and the charges are high, but the medical level is average\n",
      "tensor([[0.5096, 0.4904]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "print(\"请输入一句与医疗相关的英文：\")\n",
    "s = input() \n",
    "print(predict_sentiment(net, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38e68f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T07:59:06.899860Z",
     "start_time": "2022-03-12T07:59:04.607791Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入一句与医疗相关的英文：\n",
      "This hospital is very good. The dentist in charge of me is very patient and the treatment effect is good\n",
      "tensor([[0.2349, 0.7651]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(\"请输入一句与医疗相关的英文：\")\n",
    "s = input() \n",
    "print(predict_sentiment(net, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863f6efc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T07:59:43.961250Z",
     "start_time": "2022-03-12T07:59:41.358083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入一句与医疗相关的英文：\n",
      "It's too bad. It was just a minor illness. It hasn't been ranked in the number. It's getting worse and worse\n",
      "tensor([[9.9999e-01, 6.0150e-06]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "print(\"请输入一句与医疗相关的英文：\")\n",
    "s = input() \n",
    "print(predict_sentiment(net, s))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
